\chapter{Repérage dans $\mathbb{R}^3$}
\chapter{Fonctions de plusieurs variables}
\chapter{Dérivation en plusieurs variables}
\section{Définition et premières propriétés}
 En dimension 1, la dérivée ne peut être approchée par uniquement 2 directions (par la gauche et par la droite).
 A partir de la dimension 2, il y a infinité de directions par lesquelles approchée la dérivée.\\
 \newline
 Schéma\\
On peut commencer par dérivée selon les vecteurs de bases de $\mathbb{R}^n$ ( les vecteurs $\overrightarrow{u_n}$), les vecteurs
\begin{defi}
    La dérivée partielle de $f:\mathbb{R}^n\to\mathbb{R}$ par rapport à $x_i$, $1\ge i\ge n$, en $\overrightarrow{a}\in\mathscr{D}_f$, noté $\frac{\partial f(\overrightarrow{a})}{\partial x_i}$ ou $f^{\prime}_{x_i}(\overrightarrow{a})$ est la valeur de la limite suivante quand elle existe :
    $$\frac{\partial f(\overrightarrow{a})}{\partial x_i}=\lim_{t\to 0}\frac{f(a_1,...,a_i+t,...,a_n)-f(a_1,...,a_n)}{t}$$
\end{defi}
Dans le cas d'une fonction défini comme $f:\mathbb{R}^2\to\mathbb{R}$, on a :\\
$$\frac{\partial f(x,y)}{\partial x} = \lim_{t\to 0}\frac{f(x+t,y)-f(x,y)}{t}$$
$$\frac{\partial f(x,y)}{\partial y} = \lim_{s\to 0}\frac{f(x+s,y)-f(x,y)}{s}$$
De manière pratique, on choisit la variable par rapport à laquelle on dérive l'expression, on considère toutes les autres constantes et on dérive "normalement".
\begin{ex}
\begin{enumerate}
    \item Soit $f(\overrightarrow{x})=3x_1^3x_3+2x_1x_2^2-5x_3^4$, on a :
    $$\frac{\partial f(\overrightarrow{x})}{\partial x_1} = 9x_1^2x_3+2x_2^2 $$
    $$\frac{\partial f(\overrightarrow{x})}{\partial x_2} = 4x_1x_2$$
    $$\frac{\partial f(\overrightarrow{x})}{\partial x_3} = 3x_1^3-20x_3^3$$
    \item Soit $f(x,y)=e^{xy^2}$, on a :
    $$\frac{\partial f(x,y)}{\partial x} = y^2e^{xy^2}$$
    $$\frac{\partial f(x,y)}{\partial y} = 2xye^{xy^2}$$
\end{enumerate}
\end{ex}
Il possible de dérivée selon une direction quelconque, pour cela on définit la dérivée directionnelle.
\begin{defi}
    Soient $f:\mathbb{R}^n\to\mathbb{R}$, $\overrightarrow{a}\in\mathscr{D}_f$ et $\overrightarrow{h}\in\mathbb{R}^n$.\\
    La dérivée directionnelle de $f$ en $\overrightarrow{a}$ suivant la direction $\overrightarrow{h}$ est la quantité défini par la limite suivante si elle existe:
    $$\frac{\partial f}{\partial\overrightarrow{h}}(\overrightarrow{a})=\lim_{\epsilon\to 0}\frac{f(\overrightarrow{a}+\epsilon\overrightarrow{h})-f(\overrightarrow{a})}{\epsilon}$$ 
    \newline
\end{defi}
\subsection{Gradient et matrice jacobienne}
On définit pour les fonction
\begin{defi}
Soit un champ de vecteur $f:\mathbb{R}^n\to\mathbb{R}^p$, on considère les dérivées partielles selon chacune des variables.\\
Selon la valeur de $p$, on créé deux objets :
\begin{itemize}
    \item Si  p=1, on définit le gradient de la fonction scalaire $f$ en $\overrightarrow{a}$ : $$\overrightarrow{\text{grad}}f(\overrightarrow{a})=\overrightarrow{\nabla} f(\overrightarrow{a})=\begin{pmatrix}\frac{\partial f}{\partial x_1}(\overrightarrow{a})\\\vdots\\\frac{\partial f}{\partial x_n}(\overrightarrow{a})\end{pmatrix}$$
    \item Si $p>1$, on défini la matrice jacobienne $J_f(\overrightarrow{a})$ (de dimension $n\times p$) de la fonction $f$ en $\overrightarrow{a}$ : $$J_f(\overrightarrow{a})=\begin{pmatrix}\frac{\partial f_1}{\partial x_1}(\overrightarrow{a})&\hdots&\frac{\partial f_1}{\partial x_n}(\overrightarrow{a})\\ \vdots & \ddots & \vdots\\ \frac{\partial f_p}{\partial x_1}(\overrightarrow{a}) & \hdots & \frac{\partial f_p}{\partial x_n}(\overrightarrow{a})\end{pmatrix}$$
\end{itemize}
\end{defi}
\section{Différentielle}
\section{Dérivées d'ordres supérieurs}
\chapter{Opérateurs différentielles}
Attention, l'expression des opérateurs changent selon le système de coordonnées.
\section{Le gradient $\protect\overrightarrow{\nabla}$}
Nous avons déjà définit le gradient d'une fonction évalué en un point, on va maintenant définir l'opérateur gradient.
\begin{defi}
On appelle gradient, l'opérateur définit comme il suit :
$$\overrightarrow{\nabla}=\begin{pmatrix}\frac{\partial}{\partial x_1}\\\vdots\\\frac{\partial}{\partial x_n}\end{pmatrix}$$
\newline
Le gradient d'une fonction scalaire $f:\mathbb{R}^n\to\mathbb{R}$ est donc :
$$\overrightarrow{\nabla}f=\begin{pmatrix}\frac{\partial}{\partial x_1}\\\vdots\\\frac{\partial}{\partial x_n}\end{pmatrix}\times f=\begin{pmatrix}\frac{\partial f}{\partial x_1}\\\vdots\\\frac{\partial f}{\partial x_n}\end{pmatrix}=\overrightarrow{grad(\overrightarrow{f})}$$
\end{defi}
\begin{prop}
Soit un champ scalaire $\overrightarrow{f}$ dans l'espace $\mathbb{R}^3$, nous avons pour expression du gradient :
\begin{itemize}
    \item En coordonnées cartésiennes (par définition) : $$\overrightarrow{\nabla}f=\frac{\partial f}{\partial x}\overrightarrow{u_x}+\frac{\partial f}{\partial y}\overrightarrow{u_y}+\frac{\partial f}{\partial z}\overrightarrow{u_z}$$
    \item En coordonnées cylindriques : $$\overrightarrow{\nabla}f=\frac{1}{r}\frac{\partial f}{\partial r}\overrightarrow{u_r}+\frac{\partial f}{\partial \theta}\overrightarrow{u_\theta}+\frac{\partial f}{\partial z}\overrightarrow{u_z}$$
    \item En coordonnées sphériques : $$\overrightarrow{\nabla}f=\frac{\partial r}{\partial x}\overrightarrow{u_r}+\frac{1}{r}\frac{\partial f}{\partial \theta}\overrightarrow{u_\theta}+\frac{1}{r\sin{\theta}}\frac{\partial f}{\partial \phi}\overrightarrow{u_\phi}$$
\end{itemize}
\end{prop}
\begin{demo}
soit
\end{demo}
\section{La divergence $div$}
\begin{defi}
Soit un champ de vecteurs $\overrightarrow{f}:\mathbb{R}^n\mapsto\mathbb{R}^n$ de classe $\mathscr{C}^1$ sur $\mathscr{D}_f$.\\
La divergence de $\overrightarrow{f}$ est donné par :
$$div(\overrightarrow{f})=\overrightarrow{\nabla}.\overrightarrow{f}=\sum_{i=1}^n \frac{\partial f_i}{\partial x_i}$$
\end{defi}
\begin{rmq}
La divergence est également égale à la trace de la matrice jacobienne.
$$div(\overrightarrow{f})=\text{Tr}(J_f)$$
\end{rmq}
\begin{prop}
Soit un champ scalaire $\overrightarrow{f}$ dans l'espace $\mathbb{R}^3$, nous avons pour expression de la divergence :
\begin{itemize}
    \item En coordonnées cartésiennes (par définition) : $$div(\overrightarrow{f})=\frac{\partial f_x}{\partial x}+\frac{\partial f_y}{\partial y}+\frac{\partial f_z}{\partial z}$$
    \item En coordonnées cylindriques : $$div(\overrightarrow{f})=\frac{1}{r}\frac{\partial (r f_r)}{\partial r}+\frac{1}{r}\frac{\partial f_{\theta}}{\partial \theta}+\frac{\partial f_z}{\partial z}$$
    \item En coordonnées sphériques : $$\overrightarrow{\nabla}f=\frac{\partial r}{\partial x}\overrightarrow{u_r}+\frac{1}{r}\frac{\partial f}{\partial \theta}\overrightarrow{u_\theta}+\frac{1}{r\sin{\theta}}\frac{\partial f}{\partial \phi}\overrightarrow{u_\phi}$$
\end{itemize}
\end{prop}
\begin{thm}[Théorème de Green-Ostrogradski]

\end{thm}
\section{Le rotationnel $rot$}
\begin{defi}
Soit un champ de vecteurs $\overrightarrow{f}:\mathbb{R}^3\mapsto\mathbb{R}^3$ de classe $\mathscr{C}^1$ sur $\mathscr{D}_f$, de coordonnées\\ $f=(f_1,f_2,f_3)$.\\
Le rotationnel de $\overrightarrow{f}$ de ce champ est défini par :

$$rot(\overrightarrow{f})=\overrightarrow{\nabla}\land\overrightarrow{f}=\begin{pmatrix}\frac{\partial}{\partial x}\\\frac{\partial}{\partial y}\\\frac{\partial}{\partial z}\end{pmatrix}\land\begin{pmatrix}f_1\\f_2\\f_3\end{pmatrix}=\begin{pmatrix}\frac{\partial f_3}{\partial y}-\frac{\partial f_2}{\partial z}\\\frac{\partial f_1}{\partial z}-\frac{\partial f_3}{\partial x}\\\frac{\partial f_2}{\partial x}-\frac{\partial f_1}{\partial y}\end{pmatrix}$$
\end{defi}

\section{Le laplacien $\Delta$}
\begin{defi}
Soit une champ scalaire $f:\mathbb{R}^n\mapsto\mathbb{R}$ de classe $\mathscr{C}^2$ sur $\mathscr{D}_f$.
Le laplacien, noté $\Delta$ $\nabla^2$, est donné par :
$$\Delta \overrightarrow{f}=\nabla . \overrightarrow{\nabla} f = div(\overrightarrow{grad}f)=\sum_{i=1}^n \frac{\partial^2 f}{\partial {x_i}^2}$$
\end{defi}
\section{Équations aux dérivées partielles}
\chapter{Intégration en plusieurs variables}
\chapter{Calcul matriciel}
\begin{defi}
Une matrice $n\times m$ à  coefficients réels est un tableau de nombres réels de $n$ lignes et $m$ colonnes.\\
On note $a_{ij}$ le coefficient à la i-ème  et à la j-ème colonne.\\
On représente une matrice de la manière suivante :
$$A = \begin{pmatrix}a_{11} & a_{12} & \hdots & a_{1m}\\a_{21} & a_{22} & \hdots & a_{2m}\\ \vdots & \vdots & \ddots & \vdots\\ a_{n1} & a_{n2} & \hdots & a_{nm} \end{pmatrix}$$
\newline
Une matrice $n\times m$ à valeurs réelles appartient à l'ensemble des matrices $n\times m$, noté $\mathbb{M}^{n\times m}(\mathbb{R})$.
\end{defi}
Toute application linéaire ou système d'équation linéaire peut être écrit sous forme matricielle.
\begin{ex}
ex de base
On considère l'application
\end{ex}
\section{Calcul matriciel}
On commence par définir l'addition de deux matrices.
\begin{defi}
Soient deux matrices $A$ et $B$ de dimension $n\times m$.\\
L'addition de ces deux matrices est donnée par :
$$\begin{pmatrix}a_{11} & a_{12} & \hdots & a_{1m}\\a_{21} & a_{22} & \hdots & a_{2m}\\ \vdots & \vdots & \ddots & \vdots\\ a_{n1} & a_{n2} & \hdots & a_{nm} \end{pmatrix}+\begin{pmatrix}b_{11} & b_{12} & \hdots & b_{1m}\\b_{21} & b_{22} & \hdots & b_{2m}\\ \vdots & \vdots & \ddots & \vdots\\ b_{n1} & b_{n2} & \hdots & b_{nm} \end{pmatrix}=\begin{pmatrix}a_{11}+b_{11} & a_{12}+b_{12} & \hdots & a_{1m}+b_{1m}\\a_{21}+b_{21} & a_{22}+b_{22} & \hdots & a_{2m}+b_{2m}\\ \vdots & \vdots & \ddots & \vdots\\ a_{n1}+b_{n1} & a_{n2}+b_{n2} & \hdots & a_{nm}+b_{nm} \end{pmatrix}$$
\end{defi}
\begin{defi}
Soient deux matrices $A$ de dimension $n\times m$ et $B$ de dimension $m\times p$.\\
La multiplication de ces deux matrices nous donnera une matrice $C$ de dimension $n\times p$.\\
Le calcul des coefficients de la matrice $C$ est donné par la formule :
$$c_{ij}=\sum_{k=1}^m a_{ik}\times b_{kj}=a_{i1}b_{1j}+a_{i2}b_{2j}+\hdots+a_{in}b_{nj}$$
\end{defi}
Attention, la multiplication matricielle n'est pas commutative
\begin{ex}
On considère les matrices $A=$ et $B=$
\end{ex}
\begin{defi}
Soit une matrice $A$ de dimension $n\times m$.\\
L'opération de transposition de cette matrice nous renverra la matrice transposée de $A$, $B$ de dimension $m\times n$.\\
Cet opération est défini comme il suit :
$$A^\text{T}$$
\end{defi}
\begin{defi}
Soit une matrice $A$ de dimension $n\times m$.\\
On appelle trace de la matrice $A$ la somme de tous ses termes diagonaux.
$$\text{Tr}(A)=\sum_{i=1}^n a_{ii}$$
\end{defi}
\begin{prop}
Soient $A$ et $B$ de matrice et $\alpha\in\mathbb{R}$.\\
\begin{itemize}
    \item $\text{Tr}(A+B)=\text{Tr}(A)+\text{Tr}(B)$
    \item $\text{Tr}(\alpha A)=\alpha\text{Tr}(A)$
    \item $\text{Tr}(A^\text{T})=\text{Tr}(A)$
    \item Si $A$ et $B$ sont multipliable : $\text{Tr}(AB)=\text{Tr}(BA)$
    \item Si $A$ et $B$ sont semblables : $\text{Tr}(A)=\text{Tr}(B)$
\end{itemize}
\end{prop}
\subsection{Multiplication matricielle}
\subsection{Transposition matricielle}
\subsection{Propriétés et caractéristiques de matrices}
\section{Matrices particulières}
\subsection{Matrice carrée et rectangulaire}
\subsection{Matrice élémentaire et algorithme de Gaus-Jordan}
\subsection{Matrices de passage}
\subsection{Matrices triangulaires}
\section{Déterminants et inversion de matrices}
\subsection{Comatrice et matrice adjointe}
\chapter{Diagonalisation}
  La diagonalisation est le second "problème principal" d'algèbre, le premier étant la résolution de systèmes
  linéaires.
Diagonaliser une matrice revient à la "simplifier".\\
L'intérêt d'un tel procédé est qu'il simplifie certains calculs tel que la multiplication ou l'exponentiation.\\
La diagonalisation consiste à chercher une base $\mathscr{B}$ de l'espace, dans laquelle la matrice $A$ est diagonale.\\
Dans la suite de ce chapitre nous ne considérerons que des matrices carrées.\\
\begin{bclogo}[couleur=blue!30,couleurBord=blue,arrondi=0.1,logo=\bcbook,ombre=true]{Définition}
Une application linéaire $f:\mathbb{R}^{n}\rightarrow\mathbb{R}^{n}$  est dite diagonalisable si et seulement si $\exists \mathscr{B}$ une base de $\mathbb{R}^{n}$ tel que sa matrice représentative $A_{\mathscr{B},\mathscr{B}}(f)$ est diagonale.
\end{bclogo}
\section{Éléments propres}
Il convient dans un premier temps de définir les différents objets qui servirons à la diagonnalisation, ces objets sont appelés éléments propres.
\begin{bclogo}[couleur=blue!30,couleurBord=blue,arrondi=0.1,logo=\bcbook,ombre=true]{Définition}
Soit A une matrice $n\times n$.
\begin{itemize}
    \item[$\bullet$] On dit que $\lambda\in\mathbb{C}$ est une valeur propre de $A$ s'il existe $x\in\mathbb{C}^{n}$ avec $x\neq 0$ tel que $Ax = \lambda x$.
    \item[$\bullet$] On appelle alors le vecteur $x$ le vecteur propre de $A$ associé à la valeur propre $\lambda$.
    \item[$\bullet$] On appelle spectre de $A$ l'ensemble des valeurs propres de $A$.
    \item[$\bullet$] On appelle sous espace propre de $A$ (associé à la valeur propre $\lambda$), noté $E_{\lambda}$, l'ensemble de tous les vecteurs $x$ tel que
$Ax=\lambda x \Leftrightarrow (A-\lambda I_n)x=0$.\\
Autrement dit, $E_{\lambda}=\ker(A-\lambda I_n)=\{x\in E | Ax=\lambda x\}$\\
\end{itemize}
\end{bclogo}
\begin{bclogo}[logo=\bccrayon,noborder=true,barre=snake]{Exemple}
Soit la matrice $A=\begin{pmatrix}
5 & 2 \\
4 & 3
\end{pmatrix}$\\
Par définition d'une valeur propre $\lambda$, nous cherchons un vecteur $x$ tel que $Ax=\lambda x$.
Dans le cas présent, on a :\\
$$\begin{pmatrix} 5 & 2\\ 4 & 3\end{pmatrix}\times\begin{pmatrix}x_1\\x_2\end{pmatrix}=\lambda\times\begin{pmatrix}x_1\\x_2\end{pmatrix}$$\\
Il nous faut donc résoudre le système suivant :\\
$$\begin{cases}5x_1+2x_2=\lambda x_1\\4x_1 + 3x_2=\lambda x_2\end{cases}$$
Sa résolution nous renvoie notamment le vecteur $x=(7,7)$ et il se trouve que $Ax=7\times x$.\\
On a donc que $\lambda = 7$ une valeur propre de la matrice $A$ et $x=(1,1)$ est vecteur un propre de la matrice $A$ associé à la valeur propre $\lambda = 7$.\\
\end{bclogo}
L'utilisation de la définition d'une valeur propre pour son calcul est une opération assez fastidieuse, c'est pour cela que l'on passe par d'autres moyens pour les déterminer.\\
On utilise pour cela le polynôme caractéristique de la matrice $A$.
\section{Polynôme caractéristique et calcul des éléments propres}
Le calcul des éléments propres est plus facile en passant par le polynôme caractéristique.
\begin{bclogo}[couleur=blue!30,couleurBord=blue,arrondi=0.1,logo=\bcbook,ombre=true]{Définition}
Soit une application linéaire $f:\mathbb{R}^n\to\mathbb{R}^n$ et sa matrice représentative dans la base $\mathscr{B}$ $A_{\mathscr{B}}(f)$.
On appelle polynôme caractéristique de l'application $f$, le polynôme défini de la façon suivante :
$$P_{f}(\lambda)=P_{A}(\lambda)=\det(A-\lambda I_{n})=\begin{vmatrix}
a_{11}-\lambda & a_{12} & \hdots & a_{1n}\\
a_{21} & a_{22}-\lambda &&\vdots \\
\vdots & & \ddots & \vdots\\
a_{n1} &\hdots&\hdots& a_{nn}-\lambda
\end{vmatrix}$$
\\
\end{bclogo}

\begin{bclogo}[couleur=green!30,couleurBord=green,logo=\bccle ,ombre=true,arrondi=0.1]{Proposition}
Les valeurs propres $\lambda$ de $A$ sont les racines du polynôme caractéristique.
$$\lambda \text{ est une valeur propre de la matrice } A \Leftrightarrow P_A(\lambda) = 0$$
\end{bclogo}

\begin{bclogo}[logo=\bccrayon,noborder=true,barre=snake]{Exemple}
Soit la matrice $A$ définie comme $A=\begin{pmatrix}
7 & 4 \\
3 & 6 \\
\end{pmatrix}$.\\
On commence par chercher les valeurs propres de $A$.
Par la proposition précédente, on a :\\
$$P_A(\lambda) = | A-\lambda I_3 | =\begin{vmatrix}
7 - \lambda & 4\\
3 & 6- \lambda \\
\end{vmatrix} = (7-\lambda)(6-\lambda) -12$$\\
On cherche donc les racines de $P_A(\lambda)$.
\begin{align*}
   & P_A(\lambda)=0\\
\Leftrightarrow & {\lambda}^{2}-13\lambda +30 =0\\
\Leftrightarrow & {\lambda}_{1}=3 \text{ et } {\lambda}_{2}=10 
\end{align*}

Les valeurs propres de la matrice $A$ sont donc ${\lambda}_{1}=3 \text{ et } {\lambda}_{2}=10$
\end{bclogo}
On constate, assez aisément, que la détermination des valeurs propres à l'aide du polynôme caractéristique est beaucoup plus facile et rapide.
\begin{bclogo}[couleur=red!30,couleurBord=red,ombre=true,arrondi=0.1,logo=\bcoutil]{Propriétés}
Soit la matrice $A$ et son polynôme caractéristique $P_A(\lambda)$.
\begin{itemize}
\item Si l'on injecte $0$ dans le polynôme caractéristique il nous renverra la valeur du déterminant de cette matrice :\
$$P_A(0)=\det(A)$$
\item Le polynôme caractéristique possède $n$ racines dans l'ensemble $\mathbb{C}$
\item Deux matrices semblables ont le même polynôme caractéristique.
\item Le polynôme caractéristique d'une matrice est égale à celui de sa transposée $P_A(\lambda)=P_{A^\text{T}}(\lambda)$.
\end{itemize}
\end{bclogo}

Le polynôme caractéristique nous donne un moyen simple de déterminer les valeurs propres.\\
De ces valeurs propres, on peut déduire le reste des éléments propres de la matrice.\\
\\
Afin de pouvoir continuer sereinement, nous allons introduire les multiplicités algébriques et géométriques, qui seront utiles pour la suite.

\begin{bclogo}[couleur=blue!30,couleurBord=blue,arrondi=0.1,logo=\bcbook,ombre=true]{Définition}
\begin{itemize}
    \item On appelle multiplicité géométrique d'une valeur propre $\lambda$ : la dimension du sous espace propre associé à la valeur propre $\lambda$.
    \item On appelle multiplicité algébrique d'une valeur propre $\lambda$ : la multiplicité de $\lambda$ en tant que racine du polynôme caractéristique.
\end{itemize}
\end{bclogo}
\begin{prop}
Soit $A\in\mathbb{M}^{n\times n}$, la trace de $A$ est égale à la somme des valeurs propres multipliées avec leur multiplicité propre.
$$\text{Tr}(A)=\sum_{i=1}^n \lambda_i\times a_i$$
\end{prop}

\begin{bclogo}[logo=\bclampe,arrondi=0.1,ombre=true, couleur=yellow!60,couleurBord=yellow]{Méthode : Détermination des éléments propres}
\begin{enumerate}
    \item Déterminer le polynôme caractéristique
    \item Trouver les valeurs propres de $A$, en déduire le spectre de $A$
    \item Rechercher les vecteurs propres associés au valeurs propres $\lambda$
    \item Déterminer les sous espaces propres $E_{\lambda}$ associés aux valeurs propres $\lambda$.\\
    Pour ce faire, il suffit de trouver le noyau de la matrice $A-\lambda I_n$ $\Leftrightarrow \ker(A-\lambda I_n)$.\\
\end{enumerate}
\end{bclogo}
\begin{ex}
On se donne la matrice $A=\begin{pmatrix}3&-1&1\\0&2&0\\1&-1&3\end{pmatrix}$\\
On commence par poser $A-\lambda I_3$ :
$$A-\lambda I_n = \begin{pmatrix}3&-1&1\\0&2&0\\1&-1&3\end{pmatrix}-\begin{pmatrix}\lambda&0&0\\0&\lambda&0\\0&0&\lambda\end{pmatrix}=\begin{pmatrix}3-\lambda&-1&1\\0&2-\lambda&0\\1&-1&3-\lambda\end{pmatrix}=B$$
Le polynôme caractéristique nous est donné par le déterminant de cette nouvelle matrice $B$.
$$P_A(\lambda)=\det(A-\lambda I_3)=\det(B)=\begin{vmatrix}3-\lambda&-1&1\\0&2-\lambda&0\\1&-1&3-\lambda\end{vmatrix}$$
On choisi de développer le déterminant selon la deuxième ligne, en effet celui-ci nous serra plus facile a calculer.  On a donc :
$$P_A(\lambda)=\det(A-\lambda I_3)=(2-\lambda)((3-\lambda)^2-1)$$
On sait que les racines du polynôme caractéristique, sont les valeurs propres de $A$, donc :
\begin{align*}
    \Leftrightarrow & P_A(\lambda) = 0\\
    \Leftrightarrow & (2-\lambda)((3-\lambda)^2-1) = 0\\
    \Leftrightarrow & (2-\lambda)(\lambda^2-6\lambda+8) = 0
\end{align*}
Le premier facteur nous renvoie $\lambda = 2$ et le deuxième facteur nous donne $\lambda_1=2$ et $\lambda_2=4$.\\
On a donc les valeurs propres de $A$ qui sont: $\lambda_1 =2$ valeur propre de multiplicité algébrique $2$, $\lambda_2=4$ valeur propre de $A$ de multiplicité algébrique $1$.\\
On en déduit le spectre de $A$ : $Spec(A)=\{2;4\}$
\\
\\
On souhaite maintenant déterminer les vecteurs propres $x$ associées aux valeurs propres $\lambda$.\\
Pour une valeur propre $\lambda$ donnée, la recherche du vecteur propre associée passe par la résolution de l'égalité $(A-\lambda I_3)x=0$.\\
Pour la valeur propre $\lambda_1 = 2$ on a $(A-2I_3)x=0$.On commence par poser $A-2I_3$ :
$$A-2I_3=\begin{pmatrix}3&-1&1\\0&2&0\\1&-1&3\end{pmatrix}-\begin{pmatrix}2&0&0\\0&2&0\\0&0&2\end{pmatrix}=\begin{pmatrix}1&-1&1\\0&0&0\\1&-1&1\end{pmatrix}=C$$
Il nous faut donc résoudre :
\begin{align*}
    \Leftrightarrow & (A-2I_3)x=0\\
    \Leftrightarrow & Cx=0\\
    \Leftrightarrow & \begin{pmatrix}1&-1&1\\0&0&0\\1&-1&1\end{pmatrix}\times\begin{pmatrix}x_1\\x_2\\x_3\end{pmatrix}=\overrightarrow{0}\\
    \Leftrightarrow & \begin{cases}x_1-x_2+x3=0\\0=0\\x_1-x_2+x_3=0\end{cases}\\
    \Leftrightarrow & x_1-x_2+x_3=0\\
    \Leftrightarrow & x_1=x_2-x_3
    \Rightarrow \begin{pmatrix}x_2-x_3\\x_2\\x_3\end{pmatrix}=x
\end{align*}
On peut décomposer le vecteur :
$$\begin{pmatrix}x_2-x_3\\x_2\\x_3\end{pmatrix}=\begin{pmatrix}x_2\\x_2\\0\end{pmatrix}+\begin{pmatrix}-x_3\\0\\x_3\end{pmatrix}=x_2\times\begin{pmatrix}1\\1\\0\end{pmatrix}+x_2\times\begin{pmatrix}-1\\0\\1\end{pmatrix}$$
Il y a donc deux vecteurs propres $v_1=\begin{pmatrix}1\\1\\0\end{pmatrix}$ et $v_2=\begin{pmatrix}-1\\0\\1\end{pmatrix}$ associées a la valeur propre $\lambda_1=2$.\\
\newline
Pour la valeur propre $\lambda_2=4$ on pose $(A-4I_3)x=0$.
On effectue un raisonnement analogue au précédent :
$$A-4 I_3=\begin{pmatrix}3&-1&1\\0&2&0\\1&-1&-1\end{pmatrix}-\begin{pmatrix}4&0&0\\0&4&0\\0&0&4\end{pmatrix}=\begin{pmatrix}-1&-1&1\\0&-2&0\\1&-1&-1\end{pmatrix}=D$$
On cherche $\ker(A-4I_3)$ :
\begin{align*}
    \Leftrightarrow & \ker(A-4I_3)\\
    \Leftrightarrow & \ker D\\
    \Leftrightarrow & Dx=0\\
    \Leftrightarrow & \begin{pmatrix}-1&-1&1\\0&-2&0\\1&-1&-1\end{pmatrix}\times\begin{pmatrix}x_1\\x_2\\x_3\end{pmatrix}=\begin{pmatrix}0\\0\\0\end{pmatrix}\\
    \Leftrightarrow & \begin{cases}-x_1-x_2+x_3=0\\-2x_2=0\\x_1-x_2-x_3=0\end{cases}\\
    \Leftrightarrow & \begin{cases}-x_1+x_3=0\\ x_2=0\\x_1-x_3\end{cases}\\
    \Leftrightarrow & x_1-x_3=0\\
    \Leftrightarrow & x_1=x_3 \Rightarrow \begin{pmatrix}x_1\\0\\x_1\end{pmatrix}
\end{align*}
Ici il y a un vecteur propre associé à la valeur propre $\lambda_2=4$ qui est $v_1=\begin{pmatrix}1\\0\\1\end{pmatrix}$.\\
\newline 
Et enfin, a partir des vecteurs propres on déduit les sous espaces propres, ici :
$$E_{\lambda_1}=\text{Vect}\begin{pmatrix}\begin{pmatrix}1\\1\\0\end{pmatrix},\begin{pmatrix}-1\\0\\1\end{pmatrix}\end{pmatrix}$$

$$E_{\lambda_2}=\text{Vect}\begin{pmatrix}\begin{pmatrix}1\\0\\1\end{pmatrix}\end{pmatrix}$$
\end{ex}


\section{Diagonalisation}
Revenons sur la définition d'une matrice diagonalisable, pour en donner une définition plus mathématique.
\begin{bclogo}[couleur=blue!30,couleurBord=blue,arrondi=0.1,logo=\bcbook,ombre=true]{Définition}
La matrice $A\in\mathbb{M}^{n\times n}$ est diagonalisable, s'il existe une matrice diagonale $D$ et une matrice $P$ inversible telles que $A=PDP^{-1}$, où $P$ est la matrice de passage entre la base canonique et la base $\mathscr{B}$ où la matrice $D$ existe.
\end{bclogo}
Une propriété qui viens directement a l'esprit est que les matrices $A$ et $D$ sont semblables (par définition).\\
Tout l'enjeu sera donc de déterminer la matrice de passage $P$.\\
Néanmoins, il peut être intéressant de savoir si une matrice est diagonalisable. 
\begin{thm}[Théorème: Critère de diagonalisation]
Soit $A\in\mathbb{M}^{n\times n}(\mathbb{R})$.\\
La matrice $A$ est diagonalisable $\Leftrightarrow$
\begin{enumerate}
    \item toutes les racines $\lambda_1,\hdots,\lambda_k$ de $P_A(\lambda)$ sont contenues dans $\mathbb{R}$.
    \item $\forall i=1,\hdots,k$ on a $\text{dim}(E_{\lambda_i})=\text{multiplicité algébrique de }\lambda_i$
\end{enumerate}

\end{thm}
\begin{thm}[Corollaire]
Si $P_A(\lambda)$ admet $n$ racines réelles distinctes $\Leftrightarrow$ $A$ est diagonalisable
\end{thm}
Une fois le critère de diagonalisation ou son corollaire vérifié on peut déduire la matrice diagonale $D$ ainsi que le la matrice de passage $P$
\begin{prop}
Soit la matrice $A\in\mathbb{M}^{n\times n}$ et ses valeurs propres $\lambda_1,\hdots, \lambda_k$ et leurs multiplicités $a_1,\hdots,a_k$ associées.
La matrice diagonale $D$ est donné par :\\
$$\begin{pmatrix}
\lambda_1 & 0 & \hdots & \hdots & \hdots & \hdots & \hdots & \hdots & \hdots & 0\\
0 & \ddots & 0 & & & & & & & \vdots\\
\vdots & 0 & \lambda_1 & 0 & & & & & & \vdots\\
\vdots & & 0 & \lambda_2 & 0 & & & & & \vdots \\
\vdots & & & 0 & \ddots & 0 & & & & \vdots\\
\vdots & & & & 0 & \lambda_2 & 0 & & & \vdots\\
\vdots & & & & & 0 & \ddots & 0 & & \vdots\\
\vdots & & & & & & 0 & \lambda_k & 0 & \vdots\\
\vdots & & & & & & & 0 & \ddots & 0 \\
0 & \hdots & \hdots & \hdots & \hdots & \hdots & \hdots & \hdots & 0 & \lambda_k
\end{pmatrix}$$
où les valeurs propres $\lambda_i$ se répète autant fois que la valeur de leur multiplicité $a_i$.
\end{prop}
Nous avons la matrice diagonale, maintenant il nous faut trouver la matrice de passage $P$.
\begin{prop}

\end{prop}

L'ordre des valeurs propres n'influe pas sur le résultat, mais bien faire attention a garder l'ordre choisi sinon les calculs seront faux.
\begin{meth}[Diagonalisation d'une matrice]
\begin{enumerate}
    \item Déterminer le polynôme caractéristique
    \item Trouver les valeurs propres de $A$.\\
    $\rightarrow$ Si $\lambda_i\in\mathbb{R}$ alors $A$ est diagonalisable sinon elle ne l'est pas, on peut s'arrêter.
    \item Factoriser le polynôme caractéristique.
    \item Rechercher les sous espaces propres et leurs dimensions (multiplicité géométrique)
    \item Trouver les bases $\mathscr{B}_i$ de tous les sous espaces propres.
    \item Déduire la base $\mathscr{B}$ à partir des bases $\mathscr{B}_i$,
    ainsi que la matrice de passage a cette base.\\
    $\mathscr{B} =\bigcup_{i}^{k}\mathscr{B}_i$
\end{enumerate}
\end{meth}
\begin{ex}
rx
\end{ex}
\section{Matrices symétriques et formes quadratiques}
\begin{bclogo}[couleur=green!30,couleurBord=green,logo=\bccle ,ombre=true,arrondi=0.1]{Critère de Silvester}
Une matrice $A$ est semi-définie (définie) positive si et seulement si tous ses mineurs principales sont positifs, c'est à  dire :
$$a_{11}\ge 0, \begin{vmatrix} a_{11}&a_{12}\\ a_{21} & a_{22}\end{vmatrix}\ge 0,\begin{vmatrix}a_{11} & a_{12} & a_{13}\\ a_{21} & a_{22} & a_{23}\\ a_{31} & a_{32} & a_{33}\end{vmatrix}\ge 0, ...$$
\end{bclogo}
\chapter{Extremums en plusieurs variables}